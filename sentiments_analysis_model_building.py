# -*- coding: utf-8 -*-
"""Sentiments_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G0DEY5OAKfq80i0tpJs8c0MFm7U-X--W

We are using Amazon reviews data for sentimental analysis.In this we are using streamlit lib to create the web app that will provide the functionality of :-
* Input Text Sentiment Analysis
* Upload the csv file to get sentiments of reviews
* Also it will convet the voice into text and show it's sentiments (Speech to text)

here is the link for data:-
https://www.kaggle.com/datasets/jagdishchavan/amazon-reviews

For sentimental Analysis we are following these steps:-
* EDA of the Amazon_reviews
* Data Preprocessing
* Train-Test Split
* Feature Engineering
* Model Selection and Training
* Model Evaluation
* Hyperparameter Tuning
* Last we doing sentimental analysis for-
* user input text , by upload csv file or by speech to text
"""

# import the library

import pandas as pd
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from wordcloud import WordCloud, STOPWORDS
import string
import re
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
import nltk
from nltk.corpus import stopwords

# installing library
!pip install pyngrok
!pip install wordcloud
!pip install streamlit

# we are directly upload the amazon_reviews from side bar options. now read it by panda

data = pd.read_excel('Amazon_reviews.xlsx')
print(f"Dataset shape : {data.shape}")

"""EDA OF DATA"""

# View the top 5 row of data
data.head()

#Column names

print(f"Feature names : {data.columns.values}")

# Now we select the required column which is essential for sentimental analysis
data1 = data[['Text' , 'Score']]

data1.head()

#Check for missing values

data1.isnull().sum()

"""We found there is no missing values

"""

# Now we check for record score wise
data1.Score.value_counts()

# Now we check for duplicate values
data1[data1.duplicated()]

# Now Drop the duplicate
data2 = data1.drop_duplicates()

print(f"Dataset shape after droping the duplicate values : {data2.shape}")

# Now we analysis score column
data2.Score.value_counts()

#Bar plot to visualize the total counts of each rating

data['Score'].value_counts().plot.bar(color = 'blue')
plt.title('Rating distribution count')
plt.xlabel('Score')
plt.ylabel('Count')
plt.show()

fig = plt.figure(figsize=(4,4))
colors = ('red', 'green', 'blue','orange','yellow')
wp = {'linewidth':1, "edgecolor":'black'}
tags = data['Score'].value_counts()/data2.shape[0]
explode=(0.1,0.1,0.1,0.1,0.1)
tags.plot(kind='pie', autopct="%1.1f%%", shadow=True, colors=colors, startangle=90, wedgeprops=wp, explode=explode, label='Percentage wise distrubution of Score')
from io import  BytesIO
graph = BytesIO()
fig.savefig(graph, format="png")

data2.describe

# create the function for mapping the sentiment
def map_sentiment(Score):
    if Score > 3 :
        return 'Positive'
    elif  Score == 3:
        return 'Netural'
    else :
        return 'Negative'

data2['Review']=  data2['Score'].apply(lambda x :map_sentiment(x))

data2.head()

"""Preprocess the text"""

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
# Get the list of stopwords from NLTK corpus
stopwords_list = stopwords.words('english')
total_stopwords = set(stopwords_list)

# Identify negative stopwords and remove them from total_stopwords
negative_stop_words = set(word for word in total_stopwords
                           if "n't" in word or 'no' in word)
final_stopwords = total_stopwords - negative_stop_words

# Add the word "one" to final_stopwords
final_stopwords.add("one")

# Print the final set of stopwords
print(final_stopwords)

# removing for html tag, white space and digit from reviews

stemmer = PorterStemmer()
HTMLTAGS = re.compile('<.*?>')
table = str.maketrans (dict.fromkeys (string.punctuation))
remove_digits = str. maketrans('', '', string.digits)
MULTIPLE_WHITESPACE = re.compile(r"\s+")

def Text_preprocessor (review):
    # remove html tags
    review = HTMLTAGS. sub (r'', review)
    # remove puncutuation
    review = review.translate(table)
    # remove digits
    review = review.translate (remove_digits)
    # Lower case all letters
    review = review.lower()
    # replace multiple white spaces with single space
    review = MULTIPLE_WHITESPACE.sub(" ", review).strip()
    # remove stop words
    review = [word for word in review.split()
              if word not in final_stopwords]
    # stemming
    review =' '.join([stemmer.stem (word) for word in review])

    return review

text = 'Tea is wisdom in a cup, steeped with lessons from the ages'
Text_preprocessor (text)

# now we use wordcloud for analyse for words
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt

def generate_wcloud(text):
    stopwords = set(STOPWORDS)
    wordcloud = WordCloud(stopwords=stopwords, background_color='white')
    wordcloud.generate(text)
    plt.figure(figsize=(10, 5))
    plt.axis('off')
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.show()

# Analyze the positive text
pos = data2.loc[data2.Review == "Positive"].Text
text = " ".join(review for review in pos.astype(str))
generate_wcloud(text)

# Analyze the Negative text
Neg = data2.loc[data2. Review == "Negative"]. Text
text = " ".join(review for review in Neg.astype (str))
generate_wcloud (text)

# Analyze the Netural text
pos = data2.loc[data2.Review == "Netural"].Text
text = " ".join(review for review in pos.astype(str))
generate_wcloud(text)

#Splitting data into train and test set with 20% data with testing.
X = data2.Text
y = data2.Review
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, random_state=1, stratify=y)
X_train.shape, X_test.shape

"""Feature Engineering"""

# feature engineering using the Bag-of-Words (BoW)
bow_vectorizer = CountVectorizer (max_features=10000)
bow_vectorizer.fit(X_train)

#transform
bow_X_train = bow_vectorizer.transform(X_train)
bow_X_test = bow_vectorizer.transform(X_test)

# feature engineering using TF-IDF Vectorization
tfidf_vectorizer = TfidfVectorizer(max_features=10000)
tfidf_vectorizer.fit(X_train)
# transform
tfidf_X_train = tfidf_vectorizer.transform(X_train)
tfidf_X_test = tfidf_vectorizer.transform(X_test)

# Label Encoding for Target Variable
labelEncoder = LabelEncoder()
y_train =  labelEncoder.fit_transform(y_train)
y_test =  labelEncoder.transform(y_test)

# Get the list of classes after label encoding
labels = labelEncoder.classes_.tolist()
print(labels) # index-> class

"""Training and Evaluation of Model"""

def train_and_eval (model, trainX, trainY, testX, testY):
    # training
    model.fit(trainX, trainY)
    # predictions
    y_preds_train = model.predict(trainX)
    y_preds_test = model.predict(testX)
    # evaluation print()
    print(model)
    print (f" Train accuracy score: {accuracy_score (y_train, y_preds_train)}")
    print (f"Test accuracy score: {accuracy_score (y_test, y_preds_test)}")
    print('\n',40*'-')

# Logistic Regression Model with Hyperparameters and Count Vectorization
# Hyperparameter values for regularization strength (C)

C = [0.001, 0.01, 0.1, 1, 10]
for c in C:
    # Define model
    log_model = LogisticRegression(C=c, max_iter=500, random_state=1)
    # Train and evaluate model
    train_and_eval( model= log_model,
                   trainX=bow_X_train,
                   trainY=y_train,
                   testX=bow_X_test,
                   testY=y_test)

# Naive Bayes Model with Hyperparameters and Count Vectorization
# Hyperparameters value for alphas

alphas =[0, 0.2, 0.6, 0.8, 1]
for a in alphas:
    # Define model
    nb_model = MultinomialNB(alpha=a)
    # Train and evaluate model
    train_and_eval (model=nb_model,
                    trainX=bow_X_train,
                    trainY=y_train,
                    testX =bow_X_test,
                    testY=y_test)

# Logistic Regression Model with Hyperparameters and TF-IDF Vectorization
# Hyperparameter values for regularization strength (C)

C = [0.001, 0.01, 0.1, 1, 10]
for c in C:
    # Define model
    log_model = LogisticRegression(C=c, max_iter=500, random_state=1)
    # Train and evaluate model
    train_and_eval( model= log_model,
                   trainX=tfidf_X_train,
                   trainY=y_train,
                   testX=tfidf_X_test,
                   testY=y_test)

# Naive Bayes Model with Hyperparameters and TF-IDF Vectorization
# Hyperparameters value for alphas

alphas =[0, 0.2, 0.6, 0.8, 1]
for a in alphas:
    # Define model
    nb_model = MultinomialNB(alpha=a)
    # Train and evaluate model
    train_and_eval (model=nb_model,
                    trainX=tfidf_X_train,
                    trainY=y_train,
                    testX =tfidf_X_test,
                    testY=y_test)

"""Best Model after checking the accuracy and result"""

# Best model : Logistic Regression(C=1) with TF-IDF Vectorization
bmodel = LogisticRegression (C=1, max_iter=500, random_state=1)
bmodel.fit(tfidf_X_train, y_train)

# Making Predictions using Trained Model
# Predictions on traing data
y_preds_train = bmodel.predict(tfidf_X_train)
# Predictions on test data
y_preds_test = bmodel.predict(tfidf_X_test)

print(f"Train accuracy score: {accuracy_score (y_train, y_preds_train)}")
print (f"Test accuracy score: {accuracy_score (y_test, y_preds_test)}")

# function for confusion matrix
def plot_cm(y_true, y_pred):
    plt.figure(figsize=(6,6))
    cm = confusion_matrix(y_true, y_pred, normalize='true')
    sns.heatmap(
        cm , annot=True, cmap='Blues', cbar=False, fmt='.2f',
        xticklabels=labels, yticklabels=labels)
    return plt.show()

plot_cm(y_test, y_preds_test)

"""Now we save the model pkl file in local machine. this we will use in ide for createing web app by useing streamlit lib"""

#save and transfrom model
import pickle
with open ("transform.pkl", "wb") as f:
    pickle.dump(tfidf_vectorizer, f)
with open("sentiment-prediction-model.pkl", "wb") as f:
    pickle.dump(bmodel,f)

# Download transform.pkl

from google.colab import files
files.download("transform.pkl")

# Download sentiment-prediction-model.pkl

files.download("sentiment-prediction-model.pkl")

# predection on input review
def get_sentiment(review):
    x= Text_preprocessor(review)
    X= tfidf_vectorizer.transform([x])
    y= int(bmodel.predict(X.reshape(1,-1)))
    return labels[y]

review = "wrost product never buy it again"
print(f"this is  {get_sentiment(review)} review")

review = "product is extremly perfect. nave nice look"
print(f"this is  {get_sentiment(review)} review")

"""Here we are creating simple web app by steamlit. Main web app we create in ide"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pickle
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.preprocessing import LabelEncoder
# from sklearn.linear_model import LogisticRegression
# from nltk.stem import PorterStemmer
# import re
# import string
# 
# # Import NLTK and download necessary resources
# import nltk
# from nltk.corpus import stopwords
# 
# nltk.download('stopwords')
# 
# # Load the trained model and vectorizer
# with open("transform.pkl", "rb") as f:
#     tfidf_vectorizer = pickle.load(f)
# 
# with open("sentiment-prediction-model.pkl", "rb") as f:
#     bmodel = pickle.load(f)
# 
# # Load the label encoder and classes
# labelEncoder = LabelEncoder()
# labelEncoder.classes_ = ['Negative', 'Neutral', 'Positive']
# 
# # Load the stemming object
# stemmer = PorterStemmer()
# 
# HTMLTAGS = re.compile('<.*?>')
# table = str.maketrans(dict.fromkeys(string.punctuation))
# remove_digits = str.maketrans('', '', string.digits)
# MULTIPLE_WHITESPACE = re.compile(r"\s+")
# 
# # Define the stopwords
# stopwords_list = set(stopwords.words('english'))
# negative_stop_words = set(word for word in stopwords_list if "n't" in word or 'no' in word)
# final_stopwords = stopwords_list - negative_stop_words
# final_stopwords.add("one")
# 
# def Text_preprocessor(review):
#     review = HTMLTAGS.sub(r'', review)
#     review = review.translate(table)
#     review = review.translate(remove_digits)
#     review = review.lower()
#     review = MULTIPLE_WHITESPACE.sub(" ", review).strip()
#     review = [word for word in review.split() if word not in final_stopwords]
#     review = ' '.join([stemmer.stem(word) for word in review])
#     return review
# 
# def get_sentiment(review):
#     x = Text_preprocessor(review)
#     X = tfidf_vectorizer.transform([x])
#     y = int(bmodel.predict(X.reshape(1, -1)))
#     return labelEncoder.classes_[y]
# 
# # Streamlit app
# st.title("Sentiment Analysis App")
# 
# input_text = st.text_input("Enter your text here:")
# if st.button("Analyze"):
#     if input_text:
#         sentiment = get_sentiment(input_text)
#         st.success(f"The sentiment of the text is {sentiment}")
#     else:
#         st.warning("Please enter some text.")
#

!apt-get install -y ffmpeg

"""Address is used for Tunnel passwaord"""

! wget -q -O - ipv4.icanhazip.com

"""Click on the last Link and use the above password for redirecting to streamlit web page

"""

!streamlit run app.py & npx localtunnel --port 8501



